---
description: 《人工智能基础》课程的讲义和资源，涵盖从基础概念到前沿技术的内容。
public: true
title: 课程：《人工智能基础》
pubDate: 2025-10-03
tags:
  - Training
# author: Zhiwei Bai, Zhi-Qin John Xu, Zhangchen Zhou
# relatedWorks: f-principle/related-works
---

- # SJTU 人工智能导论
本课程 PPT 和代码参见 [Github](https://github.com/AI-Basic-Education/SJTU-Undergraduate-AI-basis)。教材参考 [此链接](https://github.com/xuzhiqin1990/understanding_dl)。
  ## 目录（Table of Contents）

  * [第1–2节 导论](#s1)
  * [第3–4节 数学基础](#s2)
  * [第5–6节 高维数据与维数灾难](#s3)
  * [第7–8节 传统监督学习](#s4)
  * [第9–10节 传统无监督学习](#s5)
  * [第11–12节 全连接与残差网络](#s6)
  * [第13–14节 卷积网络](#s7)
  * [第15–17节 循环神经网络](#s8)
  * [第18–20节 Transformer](#s9)
  * [第21–22节 训练过程中的典型现象](#s10)
  * [第23-24节 大模型简介](#s11)
  * [第25-26节 强化学习](#s12)
  * [第27-28节 可信的人工智能](#s13)

  ---

  ### 第1–2节 导论 <a id="s1"></a>

  #### 课程概览

  本节以“什么是人工智能、它从哪里来、将到哪里去”为主线，回顾 AI 的历史与里程碑，介绍机器学习/深度学习的基本概念、优化方法与“三要素”（模型、数据、优化）。

  #### 核心要点

  * AI 能做什么：识别、生成、控制、预测；线性模型、神经网络、聚类、降维、强化学习
  * 基本范式：监督学习、无监督学习、强化学习
  * 基本概念：损失函数与泛化；损失景观；常见优化器
  * 三要素：模型、数据、优化
  * 没有免费的午餐：提醒“离开数据谈算法无意义”

  ---

  ### 第3–4节 数学基础 <a id="s2"></a>

  #### 课程概览

  本节主要介绍矩阵与向量、概率统计、优化等三方面的数学基础。

  #### 核心要点

  * 线性代数：向量、矩阵、范数、相似度、SVD 与低秩近似
  * 概率统计：最大似然与经验风险，以Softmax建模概率分布，分类常用交叉熵与 KL
  * 优化：动量与自适应梯度；计算图与链式法则

  ---

  ### 第5–6节 高维数据与维数灾难 <a id="s3"></a>

  #### 课程概览

  本节围绕高维空间数据展开，内容涵盖其在自然界中的普遍性、数据本身的关键特征，以及高维性所带来的维数灾难这一核心挑战。

  #### 核心要点

  * 高维空间数据广泛存在（图像、文本、围棋）
  * 高维数据反直觉的特点：稀疏性、体积集中在表面、距离集中效应和正交性、高斯环带定理、随机投影引理
  * 缓解维数灾难的方法：蒙特卡洛方法、神经网络

  ---

  ### 第7–8节 传统监督学习 <a id="s4"></a>

  #### 课程概览

  本节系统梳理**传统监督学习**的代表模型：从**线性模型**，到**决策树**，再到**支持向量机**。

  #### 核心要点

  * 线性模型：最小二乘算法；线性分类（Logistic Regression）；LDA 的投影判别
  * 决策树：基本流程；剪枝
  * SVM：硬间隔与软间隔；核函数实现非线性分割

  ---

  ### 第9–10节 传统无监督学习 <a id="s5"></a>

  #### 课程概览

  本节系统梳理**传统无监督学习**的代表模型：**聚类**和**数据降维**。

  #### 核心要点

  * 聚类：Kmeans；高斯混合聚类
  * 降维：PCA ；等度量映射（ISOMAP） ；t-SNE 

  ---

  ### 第11–12节 全连接与残差网络 <a id="s6"></a>

  #### 课程概览

  本节介绍了全连接网络的结构以及两层神经网络的万有逼近定理，通过残差连接缓解深层网络退化与梯度消失问题。

  #### 核心要点

  * 全连接网络结构
  * 表达能力：两层网络的万有逼近能力
  * 残差连接：缓解深层网络退化问题与梯度消失问题

  ---

  ### 第13–14节 卷积神经网络 <a id="s7"></a>

  #### 课程概览

  本节先从**图像数据的统计与不变性特征**出发，再结合**生物视觉**（V1 的简单/复杂细胞与感受野）解释 CNN 的设计动机，随后介绍标准 CNN 结构及其核心算子。

  #### 核心要点

  * 图像数据特征：空间相关、频谱幂律、平移不变/等变
  * 卷积神经网络结构：卷积、零填充、步幅、池化、多通道

  ---

  ### 第15–17节 循环神经网络 <a id="s8"></a>

  #### 课程概览

  本节从**序列建模与语言任务**出发，说明语言的层次结构、长程依赖与上下文相关性对模型提出的挑战；在数据准备与词元化（BPE/WordPiece/Unigram）背景下，引入**语言模型的 Next Token Prediction（NTP）框架**。随后系统讲解**RNN 的循环计算与 BPTT 训练**、梯度消失/爆炸问题以及**LSTM 的门控机制**如何缓解长依赖困难，并展示**Encoder–Decoder**在机器翻译等可变长序列任务中的用法。

  #### 核心要点

  * 语言模型：以交叉熵训练下一个词元预测
  * 循环神经网络结构：h_t 递推与并行性的限制
  * LSTM：细胞状态与门控的稳定路径；与残差直通的类比

  ---

  ### 第18–20节 Transformer <a id="s9"></a>

  #### 课程概览

  Transformer 以**注意力（Attention）**为核心，替代 RNN 的顺序依赖，带来**全局建模能力**与**高并行效率**。最初用于机器翻译的 **Encoder–Decoder** 结构，现今更常用 **Decoder‑only + Next Token Prediction（NTP）** 的语言建模范式，并扩展到图像（ViT）与科学计算（如蛋白质结构预测）。

  #### 核心要点

  * Transformer结构组件：Embedding+位置编码，MHA（Q/K/V 与因果 Mask），FFN，残差与归一化
  * 拓展：长度外推；视觉任务与蛋白质结构建模

  ---

  ### 第21–22节 训练过程中的典型现象 <a id="s10"></a>

  #### 课程概览

  本节从**函数空间**、**参数空间**与**损失景观**三个视角，总结深度网络训练中常被反复观察到的规律：频率原则（F‑Principle）、凝聚现象（Condensation）、平坦最小值与稳定边缘现象。

  #### 核心要点

  * 频率原则：先学低频后学高频，早停可防过拟合细噪声
  * 凝聚现象：参数趋同聚集，宽网络易优化且包含窄网络解
  * 平坦性：小批量与合适较大学习率有助于到达平坦解
  * 稳定边缘现象

  ---

- ### 第23–24节 大模型简介 <a id="s11"></a>

  #### 课程概览

  本节将讨论大模型的基础架构，大模型的发展历史，大模型中的现象及应用，以及大模型推理能力的影响因素。

  #### 核心要点

  * 基础架构：语言模型（T5、BERT、GPT）、多模态模型（Clip）
  * 基本训练流程：预训练、微调和强化学习
  * 现象：规模定律（scaling law）；上下文学习（In-context learning）；思维链（CoT）；幻觉
  * 初始化对大模型推理能力的影响

  ---

- ### 第25–26节 强化学习 <a id="s12"></a>

  #### 课程概览

  本节将讨论强化学习的基本要素、强化学习的任务类型、如何解决强化学习、强化学习与神经网络如何结合。

  #### 核心要点

  * 基本要素：状态（State）；动作空间（Action space）；奖励（Reward）；期望回报（Expectation）
  * 任务类型：情节性任务、连续性任务
  * 解决强化学习方法：马尔可夫决策过程；价值函数；Bellman方程
  * 深度强化学习

  ---

* ### 第27–28节 可信的人工智能 <a id="s13"></a>

  #### 课程概览

  本节课程讨论了人工智能存在的风险以及在面对外来攻击时如何防御。

  #### 核心要点

  * 人工智能的安全性：易受攻击
  * 人工智能的攻击与防守方法：攻击方法（噪声攻击）；防守方法（对抗学习）
  * 人工智能的可解释性
